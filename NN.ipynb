{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a92325",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LOOKBACK_PERIODS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m fetch_multitimeframe_data()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Подготовка данных\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m datasets, scalers \u001b[38;5;241m=\u001b[39m create_multitimeframe_dataset(raw_data, \u001b[43mLOOKBACK_PERIODS\u001b[49m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Проверка доступности всех таймфреймов\u001b[39;00m\n\u001b[0;32m     86\u001b[0m available_tfs \u001b[38;5;241m=\u001b[39m [tf \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m TIMEFRAMES \u001b[38;5;28;01mif\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m datasets]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LOOKBACK_PERIODS' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ccxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, concatenate, Attention, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def fetch_multitimeframe_data(symbol='BTC/USDT', exchange_name='binance'):\n",
    "    \"\"\"Получение мультитаймфреймных данных с биржи\"\"\"\n",
    "    exchange = getattr(ccxt, exchange_name)()\n",
    "    timeframes = {\n",
    "        '1m': '1m',\n",
    "        '5m': '5m', \n",
    "        '15m': '15m',\n",
    "        '1h': '1h',\n",
    "        '4h': '4h',\n",
    "        '1d': '1d',\n",
    "        '1w': '1w'\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for tf in timeframes:\n",
    "        # Определение глубины исторических данных\n",
    "        since = exchange.parse8601((datetime.now() - timedelta(days=365)).isoformat())\n",
    "        \n",
    "        try:\n",
    "            # Получение OHLCV данных\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframes[tf], since=since)\n",
    "            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Фильтрация и проверка данных\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df = df.sort_index()\n",
    "            df = df.dropna()\n",
    "            \n",
    "            data[tf] = df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {tf} data: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        # Соблюдение лимитов запросов\n",
    "        time.sleep(exchange.rateLimit / 1000)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Обновленная функция создания датасетов\n",
    "def create_multitimeframe_dataset(raw_data, lookback_periods):\n",
    "    datasets = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for tf in TIMEFRAMES:\n",
    "        if tf not in raw_data:\n",
    "            continue\n",
    "            \n",
    "        # Нормализация данных\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(raw_data[tf][FEATURES])\n",
    "        scalers[tf] = scaler\n",
    "        \n",
    "        # Создание окон\n",
    "        X = []\n",
    "        for i in range(lookback_periods[tf], len(scaled_data)):\n",
    "            X.append(scaled_data[i-lookback_periods[tf]:i])\n",
    "            \n",
    "        datasets[tf] = np.array(X)\n",
    "        \n",
    "    return datasets, scalers\n",
    "\n",
    "# Модифицированный блок main\n",
    "if __name__ == \"__main__\":\n",
    "    # Получение данных с биржи\n",
    "    raw_data = fetch_multitimeframe_data()\n",
    "    \n",
    "    # Подготовка данных\n",
    "    datasets, scalers = create_multitimeframe_dataset(raw_data, LOOKBACK_PERIODS)\n",
    "    \n",
    "    # Проверка доступности всех таймфреймов\n",
    "    available_tfs = [tf for tf in TIMEFRAMES if tf in datasets]\n",
    "    if not available_tfs:\n",
    "        raise ValueError(\"No data available for any timeframe\")\n",
    "    \n",
    "    # Создание и обучение модели\n",
    "    model = create_model()\n",
    "    \n",
    "    # Подготовка данных для обучения\n",
    "    X_train = {tf: data[:-100] for tf, data in datasets.items()}\n",
    "    y_train = {\n",
    "        'price_prediction': datasets['1m'][LOOKBACK_PERIODS['1m']:-100, -1, 3],\n",
    "        'signal': (datasets['1m'][LOOKBACK_PERIODS['1m']+1:-99, 3] > \n",
    "                  datasets['1m'][LOOKBACK_PERIODS['1m']:-100, 3]).astype(int)\n",
    "    }\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Пример прогнозирования\n",
    "    current_data = {tf: data[-100:] for tf, data in datasets.items()}\n",
    "    price_pred, signal_prob = model.predict(current_data)\n",
    "    signal = generate_signal(\n",
    "        current_price=raw_data['1m'].iloc[-1]['close'],\n",
    "        predicted_price=price_pred[-1][0],\n",
    "        threshold=0.002\n",
    "    )\n",
    "    print(f\"Predicted signal: {signal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04163bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xndrf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\xndrf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
